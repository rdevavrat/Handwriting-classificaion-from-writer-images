{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #importing essentials\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_diff(z1): #method for creating the dataset\n",
    "    writer1 = []\n",
    "    writer2 = []\n",
    "    feat_id = []\n",
    "    x = []\n",
    "    y = []\n",
    "    target = []\n",
    "    diff_pair = pd.read_csv(z1)\n",
    "    feat = pd.read_csv(\"HumanObserved-Features-Data/HumanObserved-Features-Data.csv\")\n",
    "    feat_id = feat.img_id\n",
    "    x = diff_pair.img_id_A #seperating the columns from dataset\n",
    "    y = diff_pair.img_id_B #same as above\n",
    "    tar = diff_pair.target #same as above\n",
    "    \n",
    "    for i in range(0, 790):\n",
    "        a = random.randint(1, 293031)\n",
    "        writer1.append(x.iloc[a])  #choosing randoms samples\n",
    "        writer2.append(y.iloc[a])\n",
    "        target.append(tar.iloc[a])\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "\n",
    "    for i in range(len(writer1)):\n",
    "        for j in range(len(feat_id)): \n",
    "            if(writer1[i] == feat_id[j]): #comparing the sample and fetching their features\n",
    "                l1.append(feat.iloc[j, 2:11])\n",
    "            if(writer2[i] == feat_id[j]):\n",
    "                l2.append(feat.iloc[j, 2:11])   \n",
    "    feat_mat_sub = np.zeros([790, 9])\n",
    "    feat_mat_con = np.zeros([790, 18])\n",
    "    l1 = np.array(l1).reshape(790, 9)\n",
    "    l2 = np.array(l2).reshape(790, 9)\n",
    "    for k in range(0, 790):\n",
    "        for m in range(0, 9):\n",
    "            feat_mat_sub[k][m] = abs(l1[k][m] - l2[k][m])  #feature subtraction\n",
    "    feat_mat_sub = np.array(feat_mat_sub).reshape(790, 9)\n",
    "    feat_mat_con = np.concatenate((l1, l2), axis=1)  #feature concatenation\n",
    "    targ = np.array(target).reshape(790, 1) \n",
    "    print(feat_mat_sub.shape)\n",
    "    print(feat_mat_con.shape)\n",
    "    print(targ.shape)\n",
    "    return feat_mat_sub, feat_mat_con, targ\n",
    "###############################################################################################################################\n",
    "def get_dataset_same(z1): #dataset create for same pairs\n",
    "    writer1 = []\n",
    "    writer2 = []\n",
    "    feat_id = []\n",
    "    x = []\n",
    "    y = []\n",
    "    target = []\n",
    "    diff_pair = pd.read_csv(z1)\n",
    "    feat = pd.read_csv(\"HumanObserved-Features-Data/HumanObserved-Features-Data.csv\")\n",
    "    feat_id = feat.img_id\n",
    "    x = diff_pair.img_id_A\n",
    "    y = diff_pair.img_id_B\n",
    "    tar = diff_pair.target\n",
    "    for i in range(0, 790):\n",
    "        writer1.append(x.iloc[i])\n",
    "        writer2.append(y.iloc[i])\n",
    "        target.append(tar.iloc[i])\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "\n",
    "    for i in range(len(writer1)):\n",
    "        for j in range(len(feat_id)):\n",
    "            if(writer1[i] == feat_id[j]):\n",
    "                l1.append(feat.iloc[j, 2:11])\n",
    "            if(writer2[i] == feat_id[j]):\n",
    "                l2.append(feat.iloc[j, 2:11]) \n",
    "    feat_mat_sub = np.zeros([790, 9], dtype=float)\n",
    "    feat_mat_con = np.zeros([790, 18], dtype=float)\n",
    "    l1 = np.array(l1).reshape(790, 9)\n",
    "    l2 = np.array(l2).reshape(790, 9)\n",
    "    for k in range(0, 790):\n",
    "        for m in range(0, 9):\n",
    "            feat_mat_sub[k][m] = abs(l1[k][m] - l2[k][m])\n",
    "    feat_mat_sub = np.array(feat_mat_sub).reshape(790, 9)\n",
    "    feat_mat_con = np.concatenate((l1, l2), axis=1)\n",
    "    targ = np.array(target).reshape(790, 1)\n",
    "    print(feat_mat_sub.shape)\n",
    "    print(feat_mat_con.shape)\n",
    "    print(targ.shape)\n",
    "    return feat_mat_sub, feat_mat_con, targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_diff_gsc(z1, nj): #dataset create for gsc dataset using same technique as above\n",
    "    writer1 = []\n",
    "    writer2 = []\n",
    "    feat_id = []\n",
    "    x = []\n",
    "    y = []\n",
    "    target = []\n",
    "    diff_pair = pd.read_csv(z1)\n",
    "    feat = pd.read_csv(\"GSC-Features-Data/GSC-Features.csv\")\n",
    "    feat_id = feat.img_id\n",
    "    x = diff_pair.img_id_A\n",
    "    y = diff_pair.img_id_B\n",
    "    tar = diff_pair.target\n",
    "    print(len(x))\n",
    "    if(nj == 0):\n",
    "        for i in range(0, 2000):\n",
    "            a = random.randint(1, 762557)\n",
    "            writer1.append(x.iloc[a])\n",
    "            writer2.append(y.iloc[a])\n",
    "            target.append(tar.iloc[a])\n",
    "    else:\n",
    "        for i in range(0, 2000):\n",
    "            a = random.randint(1, 71532)\n",
    "            writer1.append(x.iloc[a])\n",
    "            writer2.append(y.iloc[a])\n",
    "            target.append(tar.iloc[a])\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "\n",
    "    for i in range(len(writer1)):\n",
    "        for j in range(len(feat_id)):\n",
    "            if(writer1[i] == feat_id[j]):\n",
    "                l1.append(feat.iloc[j, 1:513])\n",
    "            if(writer2[i] == feat_id[j]):\n",
    "                l2.append(feat.iloc[j, 1:513])   \n",
    "    feat_mat_sub = np.zeros([2000, 512], dtype=np.float32)\n",
    "    feat_mat_con = np.zeros([2000, 512], dtype=np.float32)\n",
    "    l1 = np.array(l1).reshape(2000, 512)\n",
    "    l2 = np.array(l2).reshape(2000, 512)\n",
    "    for k in range(0, 2000):\n",
    "        for m in range(0, 512):\n",
    "            feat_mat_sub[k][m] = abs(l1[k][m] - l2[k][m])\n",
    "    feat_mat_sub = np.array(feat_mat_sub).reshape(2000, 512)\n",
    "    feat_mat_con = np.concatenate((l1, l2), axis=1)\n",
    "    targ = np.array(target).reshape(2000, 1)\n",
    "    print(feat_mat_sub.shape)\n",
    "    print(feat_mat_con.shape)\n",
    "    print(targ.shape)\n",
    "    return feat_mat_sub, feat_mat_con, targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the datasets\n",
    "feat_mat_sub_diff, feat_mat_con_diff, target_val_diff = get_dataset_diff(\"HumanObserved-Features-Data/diffn_pairs.csv\")\n",
    "feat_mat_sub_same, feat_mat_con_same, target_val_same = get_dataset_same(\"HumanObserved-Features-Data/same_pairs.csv\")\n",
    "feat_mat_sub_gsc_diff, feat_mat_con_gsc_diff, target_val_gsc_diff = get_dataset_diff_gsc(\"GSC-Features-Data/diffn_pairs.csv\", 0)\n",
    "feat_mat_sub_gsc_same, feat_mat_con_gsc_same, target_val_gsc_same = get_dataset_diff_gsc(\"GSC-Features-Data/same_pairs.csv\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mat_sub_human = np.transpose(np.concatenate((feat_mat_sub_diff, feat_mat_sub_same))) #joining human dataset for feature subtraction\n",
    "feat_mat_con_human = np.transpose(np.concatenate((feat_mat_con_diff, feat_mat_con_same))) #joining human dataset for feature concat\n",
    "target_val_human = np.concatenate((target_val_diff, target_val_same))\n",
    "\n",
    "new1 = np.append(np.transpose(feat_mat_sub_human), target_val_human, 1)\n",
    "#print(new1.shape)\n",
    "np.random.shuffle(new1) #join target and shuffling data\n",
    "target_val_human_sub = new1[:,9] #remove target\n",
    "feat_mat_sub_human = np.transpose(np.delete(new1, 9, axis = 1))\n",
    "\n",
    "new2 = np.append(np.transpose(feat_mat_con_human), target_val_human, 1)\n",
    "#print(new2.shape)\n",
    "np.random.shuffle(new2)\n",
    "target_val_human_con = new2[:,18]\n",
    "feat_mat_con_human = np.transpose(np.delete(new2, 18, axis = 1))\n",
    "#################################################################################################################################\n",
    "feat_mat_sub_gsc = np.transpose(np.concatenate((feat_mat_sub_gsc_diff, feat_mat_sub_gsc_same))) #join gsc dataset feature sub\n",
    "feat_mat_con_gsc = np.transpose(np.concatenate((feat_mat_con_gsc_diff, feat_mat_con_gsc_same))) #join gsc dataset feature concat\n",
    "target_val_gsc = np.concatenate((target_val_gsc_diff, target_val_gsc_same))\n",
    "\n",
    "new3 = np.append(np.transpose(feat_mat_sub_gsc), target_val_gsc, 1)\n",
    "#print(new3.shape)\n",
    "np.random.shuffle(new3) #shuffle data \n",
    "target_val_gsc_sub = new3[:,512]\n",
    "feat_mat_sub_gsc = np.transpose(np.delete(new3, 512, axis = 1))\n",
    "\n",
    "new4 = np.append(np.transpose(feat_mat_con_gsc), target_val_gsc, 1)\n",
    "#print(new4.shapepe)\n",
    "np.random.shuffle(new4)\n",
    "target_val_gsc_con = new4[:,1024]\n",
    "feat_mat_con_gsc = np.transpose(np.delete(new4, 1024, axis = 1))\n",
    "#################################################################################################################################\n",
    "print(feat_mat_sub_human.shape)\n",
    "print(target_val_human_sub.shape)\n",
    "print(feat_mat_con_human.shape)\n",
    "print(target_val_human_con.shape)\n",
    "print(feat_mat_sub_gsc.shape)\n",
    "print(target_val_gsc_sub.shape)\n",
    "print(feat_mat_con_gsc.shape)\n",
    "print(target_val_gsc_con.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 1024):\n",
    "    feat_mat_con_gsc[i][i] = 0.01  #add noise \n",
    "        \n",
    "for j in range(0, 512):\n",
    "    feat_mat_sub_gsc[j][j] = 0.01 #add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "train_percent = 80\n",
    "val_percent = 10\n",
    "test_percent = 10\n",
    "M = 10\n",
    "phi = []\n",
    " \n",
    "def generateTrainingData(rawData, train_percent): #training data \n",
    "    train_len = int(math.ceil(len(rawData[0])*(train_percent*0.01)))\n",
    "    data_tr = rawData[:, 0:train_len]\n",
    "    return data_tr\n",
    "\n",
    "def generateTrainingTarget(rawTarget, train_percent=80): #training target\n",
    "    TrainingLen = int(math.ceil(len(rawTarget)*(train_percent*0.01)))  \n",
    "    t = rawTarget[:TrainingLen]\n",
    "    return t\n",
    "\n",
    "def generateValData(rawData, val_percent, TrainingCount): #validation and test data\n",
    "    valSize = int(math.ceil(len(rawData[0])*val_percent*0.01))\n",
    "    V_End = TrainingCount + valSize \n",
    "    dataMatrix = rawData[:, TrainingCount+1:V_End]\n",
    "    return dataMatrix\n",
    "\n",
    "def generateValTarget(rawData, val_percent, TrainingCount): #validation and test target\n",
    "    valSize = int(math.ceil(len(rawData)*val_percent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    return t\n",
    " \n",
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent): #big sigma generation\n",
    "    print(len(Data))\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))       \n",
    "    varVect     = []\n",
    "    \n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])\n",
    "            #print(len(vct))\n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    BigSigma = np.dot(200,BigSigma)\n",
    "    return BigSigma\n",
    "    print(\"Step Done\")\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):\n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80): #design matrix generate\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))   \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix)))\n",
    "    BigSigInv = np.linalg.inv(BigSigma) \n",
    "#     print(MuMatrix.shape)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            #print(DataT[R].shape)\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    return PHI\n",
    "    print(\"Step Done\")\n",
    "\n",
    "def GetWeightsClosedForm(PHI, T, Lambda): #weights intialization\n",
    "    Lambda_I = np.identity(len(PHI[0]))\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    PHI_T       = np.transpose(PHI)\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)\n",
    "    W           = np.dot(INTER, T)\n",
    "    return W\n",
    "\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    return Y\n",
    "\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct): #erms method\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(dat, tar, wh, m):\n",
    "    training_set = generateTrainingData(dat, train_percent) #split training data \n",
    "    training_target = np.array(generateTrainingTarget(tar, train_percent)) #split training target\n",
    "\n",
    "    validation_set = generateValData(dat, val_percent, len(training_target)) #split validation data\n",
    "    validation_target = np.array(generateValTarget(tar, val_percent, len(training_target))) #split validation target\n",
    "\n",
    "    test_data = generateValData(dat, test_percent, (len(training_target)+len(validation_target))) #split testing data\n",
    "    test_target = np.array(generateValTarget(tar, test_percent, (len(training_target)+len(validation_target)))) #split testing target\n",
    "    \n",
    "    C_Lambda = 0.01\n",
    "    kmeans = KMeans(n_clusters=m, random_state=0).fit(np.transpose(training_set)) #clusters \n",
    "    #print(\"Step 1 Done\")\n",
    "    Mu = kmeans.cluster_centers_\n",
    "    #print(\"Step 2 Done\")\n",
    "    BigSigma = GenerateBigSigma(dat, Mu, train_percent)\n",
    "    #print(\"Step 3 Done\")\n",
    "    TRAINING_PHI = GetPhiMatrix(dat, Mu, BigSigma, train_percent)\n",
    "    #print(\"Step 4 Done\")\n",
    "    #print(\"training phi\" +str(TRAINING_PHI.shape))\n",
    "    W = GetWeightsClosedForm(TRAINING_PHI,training_target,(C_Lambda)) \n",
    "    #print(\"Step 5 Done\")\n",
    "    W_Now = np.dot(220, W)\n",
    "    #print(\"W_NOw shape\" +str(W_Now.shape))\n",
    "    La = 0.2\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "    W_Mat        = []\n",
    "\n",
    "    TEST_PHI     = GetPhiMatrix(test_data, Mu, BigSigma, 100) \n",
    "    VAL_PHI = GetPhiMatrix(validation_set, Mu, BigSigma, 100)\n",
    "    for i in range(0,1000):\n",
    "\n",
    "        #stochastic gradient descent\n",
    "        Delta_E_D     = -np.dot((training_target[i] - np.dot(W_Now,np.transpose(TRAINING_PHI[i]))),TRAINING_PHI[i])\n",
    "        La_Delta_E_W  = np.dot(La,W_Now) #Lamda . delta_e_w\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learning_rate,Delta_E)\n",
    "        W_T_Next      = W_Now + Delta_W #update weights \n",
    "        W_Now         = W_T_Next\n",
    "\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,training_target)\n",
    "        L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,validation_target)\n",
    "        L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "\n",
    "        TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,test_target)\n",
    "        L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "        #print(i)\n",
    "    \n",
    "    if(wh == 0):\n",
    "        print(\"Erms for Human-Observed Feature Subtraction\")\n",
    "    elif(wh == 1):\n",
    "        print(\"Erms for Human-Observed Feature Concatenation\")\n",
    "    elif(wh == 2):\n",
    "        print(\"Erms for GSC Feature Subtraction\")\n",
    "    else:\n",
    "        print(\"Erms for GSC Feature Concatenation\")\n",
    "    print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "    print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "    print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression(feat_mat_sub_human, target_val_human_sub, 0, 8)\n",
    "linear_regression(feat_mat_con_human, target_val_human_con, 1, 17)\n",
    "linear_regression(feat_mat_sub_gsc, target_val_gsc_sub, 2, 12)\n",
    "linear_regression(feat_mat_con_gsc, target_val_gsc_con, 3, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "def sigmoid_fun(x): #sigmoid function\n",
    "    n = 1 / (1 + np.exp(-x))\n",
    "    return n\n",
    "\n",
    "def loss_fun(h, y): #loss function \n",
    "    n = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    return n\n",
    "\n",
    "def logistic_regression(X, y):\n",
    "    y = np.array(y).reshape(len(y), 1)\n",
    "    iteration = 1000\n",
    "    learning_rate = 0.01\n",
    "    sh = (X.shape[1], 1)\n",
    "    weights = np.zeros(sh)\n",
    "    for i in range(iteration):\n",
    "        x = np.dot(X, weights) #weight.T * data\n",
    "        #print(x)\n",
    "        h = sigmoid_fun(x)\n",
    "        gd = np.asarray(np.dot(X.T, (h - y)) / y.shape[0]) #calculating gradient descent\n",
    "#         print(type(gd)) \n",
    "#         print(type(weights))\n",
    "        weights -= learning_rate * gd\n",
    "        #print(loss_fun(h, y))\n",
    "    #print(weights)\n",
    "    ctr = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        c = np.dot(X, weights)\n",
    "        y_cap = sigmoid_fun(c)\n",
    "        y_cap = np.around(y_cap) #check if predicted output is same as target vector\n",
    "        if(y_cap[i] == y[i]):\n",
    "            ctr += 1\n",
    "    acc = (ctr / X.shape[0]) * 100 #calculate accuracy\n",
    "    return acc\n",
    "    \n",
    "print('Accuracy - Human Feature Subtraction ' + str(logistic_regression(np.transpose(feat_mat_sub_human), target_val_human_sub)))    \n",
    "print('Accuracy - Human Feature Concatenation ' + str(logistic_regression(np.transpose(feat_mat_con_human), target_val_human_con)))\n",
    "print('Accuracy - GSC Feature Subtraction ' + str(logistic_regression(np.transpose(feat_mat_sub_gsc), target_val_gsc_sub)))\n",
    "print('Accuracy - GSC Feature Concatenation ' + str(logistic_regression(np.transpose(feat_mat_con_gsc), target_val_gsc_con)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network\n",
    "from keras.models import Sequential #essentials\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "def get_model(first_dense_layer_nodes, second_dense_layer_nodes, input_size, drop_out): #defining a 3 layer model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(drop_out))\n",
    "    \n",
    "    model.add(Dense(second_dense_layer_nodes))\n",
    "    model.add(Activation('sigmoid')) #since the output is a classification we use a sigmoid activation\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy', #binary crossentropy since the output is either a 1 or 0\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nn(dat, tar):\n",
    "    input_size = dat.shape[0]\n",
    "    drop_out = 0.2\n",
    "    first_dense_layer_nodes  = 128\n",
    "    second_dense_layer_nodes = 1\n",
    "    model = get_model(first_dense_layer_nodes, second_dense_layer_nodes, input_size, drop_out)\n",
    "    validation_data_split = 0.2\n",
    "    num_epochs = 10000\n",
    "    model_batch_size = 128\n",
    "    tb_batch_size = 32\n",
    "    early_patience = 500\n",
    "\n",
    "    tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "    earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min')\n",
    "\n",
    "    history = model.fit(np.transpose(dat)\n",
    "                        , tar\n",
    "                        , validation_split=validation_data_split\n",
    "                        , epochs=num_epochs\n",
    "                        , batch_size=model_batch_size\n",
    "                        , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                       )\n",
    "\n",
    "    %matplotlib inline \n",
    "    df = pd.DataFrame(history.history) #plot accuracy & loss graphs\n",
    "    df.plot(subplots=True, grid=True, figsize=(10,15))\n",
    "    \n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    xs = 0.8*dat.shape[1]\n",
    "    xs = np.around(xs).astype(int)\n",
    "    test_data = dat[:,xs:]\n",
    "    test_tar = tar[xs:]\n",
    "    ne, score = model.evaluate(np.transpose(test_data), test_tar, batch_size=128) #check accuracy\n",
    "    return (score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy for human feature subtraction \" + str(_nn(feat_mat_sub_human, target_val_human_sub)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
